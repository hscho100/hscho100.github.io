---
title: Neural Discrete Representation Learning
key: 20220723
tags: Papers
---

# Main Contributions
1. A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient method.
2. For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

![vae_graph](/assets/vae_graph.png)

# VQ-VAE

- The posterior and prior distributions are categorical, and the samples drawn from these distributions index an embedding table.
- These embeddings are then used as input into the decoder network.

## Notations

- $e\in \mathbb{R}^{K\times D}}$, $K$: the size of the discrete latent space, $D$: the dimension of each latent embedding vector $e_i$.
- ed

## Discrete Latent variables

$$
q(z=k\lvert x)= \begin{cases} 1 \\ 0 \end{cases}
$$

## 어려운 점들
1. Intractibility(보통 likelihood인 $p_\theta(x\|z)$가 복잡한 함수인 경우)
  - $p_\theta(x)=\int p_\theta(z)p_\theta(x\|z)$ 적분, or $\theta$에 대한 미분이 힘든 경우
  - posterior $p_\theta(z\|x)$ 가 intractable
  - mean-field VB algorithm에서 필요한 적분이 intractable한 경우
2. Large dataset
  - Sampling-based solution(e.g. Monte Carlo EM) 너무 느림(데이터하나 마다 샘플링을 하기 위한 반복루프필요)


<!--more-->

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
