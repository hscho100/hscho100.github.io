---
layout: article
title: "NICE: Non-linear Indepedent Components Estimation"
tags: Papers
key: 20220827
mode: immersive
excerpt_separator: <!--more-->
excerpt_type : html
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .8), rgba(139, 34, 139, .8))'
    src: "https://user-images.githubusercontent.com/43431848/187059515-60104c8c-c6a0-407a-bf1e-d3cc9e14f47b.png"
---

# Abstract

Complex high-dimensional densities을 모델링하는 NICE를 개발하였다. Good representation은 모델링하기 쉬운 분포를 가졌을 것이라는 믿음(아이디어)에서 개발되었다. 비선형 deterministic 변환을 통해 데이터를 factorized distribution을 갖는 latent space로 변환한다(이는 independent component를 만들기 위함). 이 논문에서는 "이러한 변환"을 Jacobian과 Inverse Jacobian의 determinant를 구하기 쉬운 형태로 모델링을 하고, 그런 계산을 쉽게 함에도 그 모델링한 변환은 충분히 complex하고 좋은 변환이 되게 하였다. 훈련 criterion은 tractable & exact log-likelihood로 하였으며, unbiased ancestral sampling도 쉽게 할 수 있게 되었다. 마지막으로 실험에서 4가지 이미지 데이터 셋에서 좋은 생성결과를 냈으며 inpainting에서도 쓰일 수 있다는것도 보여줬다.


<img width="598" alt="image" src="https://user-images.githubusercontent.com/43431848/187059515-60104c8c-c6a0-407a-bf1e-d3cc9e14f47b.png">

<!--more-->

# Learning Bijective transfromations of continuous probabilities

$\{ p_\theta, \theta\in \Theta \}$를 finite dataset $N$ samples를 통해서 확률분포를 학습하자. 각 데이터 샘플은 $\mathcal{X}=\mathbb{R}^D$에 속한다고 하자.

$p_H$ prior distribution(predefined density function, e.g. standard isotropic Gaussian)로 부터 다음의 change of variable을 통해서 다음의 Maximum Likelihood를 달성하고자 하였다.

$$
\log (p_X(x)) = \log(p_H (f(x))) + \log(\lvert \det (\dfrac{\partial f(x)}{\partial x})\lvert )
$$



# Architecture

## Triangular Structure

## Coupling Layer

triangular Jacobian을 갖는 bijective transformation을 알아보자!(triangular이면 determinant계산이 대각원소의 곱이 되므로 쉽게 계산할 수 있다.)

### General coupling layer

![image](https://user-images.githubusercontent.com/43431848/187066801-81d40f62-3620-4391-8b2d-0755576b3fe0.png)


### Additive coupling layer


### Combining coupling layers

위에서 알아본 coupling layers를 함성하여 충분히 복잡한(complex) transformation을 만들수 있다. 이 논문에서 Jacobian을 관찰한 결과로 최소한 3개의 coupling layer가 있어야 모든 dimension이 다른 dimension에 영향을 줄수 있는 것을 확인 하였다. 이런 맥락에서 이 논문에서는 4개의 coupling layer를 사용하였다.


## Allowing Rescaling

- Additive coupling layers들은 unit Jacobian determinant(volume preserving)이므로 모델의 복잡성을 주기 위해 Diagonal Scaling Matrix $S$를 top layer에 포함시켜서 $(x_i) \rightarrow (S_{ii} x_i)$ 를 통해서 각 dimension의 중요도를 이러한 weight $S$를 통하여 반영할 수 있게 하였다.



### 결국 InfoGAN은 다음의 Minimax game with a variational regularization of mutual information and a hyperparameter $\lambda$

$$
\min_{G,Q} \max_{D} V_{\mathrm{InfoGAN}}(D,G,Q) = V(D,G)-\lambda L_I(G,Q)
$$




# Experiments


## Log-likelihood and Image generation

![image](https://user-images.githubusercontent.com/43431848/187066722-fa525483-2b27-40f3-9e15-4a9dafa5e2f8.png)

![image](https://user-images.githubusercontent.com/43431848/187066731-7e92e365-cb19-47b3-afb0-16dd3c841492.png)


## Inpainting

![image](https://user-images.githubusercontent.com/43431848/187066753-ebf9d093-d4a0-40cd-946a-911f10162a1d.png)


# Conclusion

- 학습가능한 highly non-linear bijective 변환을 통해서 학습데이터의 분포를 factorized 분포를 갖는 공간으로 변환하는 모델을 제시하였다.
- 이는 MLE를 통해서 이루어진다.
- NICE는 efficent unbiased ancestral sampling을 할 수 있으며 이를 통해 높은 log-likelihood를 달성할 수 있었다.
- NICE를 통해 more complex family of approximate posterior를 학습할 수 있으며 더 다양한 prior를 이용할 수 있는 장점이 있다. 

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
