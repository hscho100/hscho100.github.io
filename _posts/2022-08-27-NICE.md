---
layout: article
title: "NICE: Non-linear Indepedent Components Estimation"
tags: Papers
key: 20220827
mode: immersive
excerpt_separator: <!--more-->
excerpt_type : html
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .8), rgba(139, 34, 139, .8))'
    src: "https://user-images.githubusercontent.com/43431848/187059515-60104c8c-c6a0-407a-bf1e-d3cc9e14f47b.png"
---

# Abstract

Complex high-dimensional densities을 모델링하는 NICE를 개발하였다. Good representation은 모델링하기 쉬운 분포를 가졌을 것이라는 믿음(아이디어)에서 개발되었다. 비선형 deterministic 변환을 통해 데이터를 factorized distribution을 갖는 latent space로 변환한다(이는 independent component를 만들기 위함). 이 논문에서는 "이러한 변환"을 Jacobian과 Inverse Jacobian의 determinant를 구하기 쉬운 형태로 모델링을 하고, 그런 계산을 쉽게 함에도 그 모델링한 변환은 충분히 complex하고 좋은 변환이 되게 하였다. 훈련 criterion은 tractable & exact log-likelihood로 하였으며, unbiased ancestral sampling도 쉽게 할 수 있게 되었다. 마지막으로 실험에서 4가지 이미지 데이터 셋에서 좋은 생성결과를 냈으며 inpainting에서도 쓰일 수 있다는것도 보여줬다.


<img width="598" alt="image" src="https://user-images.githubusercontent.com/43431848/187059515-60104c8c-c6a0-407a-bf1e-d3cc9e14f47b.png">

<!--more-->

# Key Novelty 

"each determinant of the Jacobian" and "easy inverse"를 계산하기 쉽게 하면서도 transformation의 capacity를 충분히 높인 것이다. 그 방법은 다음과 같다.

$$
\begin{align*}
y_1 &= x_1 \\
y_2 &= x_2 + m(x_1)
\end{align*}
$$

$$
\begin{align*}
x_1 &= y_1 \\
x_2 &= y_2 - m(y_1)
\end{align*}
$$

# Learning bijective transfromations of continuous probabilities

$\{ p_\theta, \theta\in \Theta \}$를 finite dataset $N$ samples를 통해서 확률분포를 학습하자. 각 데이터 샘플은 $\mathcal{X}=\mathbb{R}^D$에 속한다고 하자.

$p_H$ prior distribution(predefined density function, e.g. standard isotropic Gaussian)로 부터 다음의 change of variable을 통해서 다음의 Maximum Likelihood를 달성하고자 하였다.

$$
\log (p_X(x)) = \log(p_H (f(x))) + \log(\lvert \det (\dfrac{\partial f(x)}{\partial x})\lvert )
$$



# Architecture

## Triangular Structure

$f=f_L \circ \cdots \circ f_1$로 구성하고 $f_i$들을 Jacobian determinant랑 inverse Jaconbian determinant를 계산하기 쉬운 걸로 구성하기 위해 대각행렬(diagonal matrix)나 diagonal matrix with rank-1 correction($D+uu^\top$ 꼴)과 삼각행렬들 을 이용함. 아래 공식 참조!

**Sherman-Morrison formula** $A$: invertible, $u,v$: column vectors

$$
(A+uv^\top)^{-1} = A^{-1} - \dfrac{A^{-1}uv^\top A^{-1}}{1+v^\top A^{-1}u}
$$

그리고 또한 중요한 사실 중의 하나가 "임의의 정사각행렬 $A$"에 대하여 $A$의 행들을 적절하게 row permutation(row shuffle)해주면 **항상** 하삼각행렬 $L$과 상삼각행렬 $U$로 곱할 수 있다는 것이다. 즉

**Remark** Let $A$ be a square matrix. Then, there exists a permutation matrix $P$, a lower triangular matrix $L$, and an upper triangular matrix $U$ that satisfy the following.
$$
PA = LU
$$

이러한 사실들을 종합해볼 때 $f=f_L \circ \cdots \circ f_1$ 식으로 $f$를 구성할 때, 각 $f_i$들을 대각행렬, 하삼각행렬, 상삼각행렬 등으로 구성시켜서 $f$를 합성하는 것이 제한적인 표현이 아니라 꽤 많은 행렬들을 표현해낼 수 있을것으로 기대할 수 있다!!!!!

**그러나** 이 논문에서 ```nn.Linear(dim,dim)```의 weight matrix를 상삼각이나 하삼각으로 하는 접근보다는 변환 $f_i$에 대하여 이 변환의 Jacobian $\dfrac{\partial f}{\partial x}$를 삼각행렬으로 가지는 $f_i$들을 만드는 전략을 취하였다! 


## Coupling Layer

triangular Jacobian을 갖는 bijective transformation을 알아보자!(triangular이면 determinant계산이 대각원소의 곱이 되므로 쉽게 계산할 수 있다.)

![image](https://user-images.githubusercontent.com/43431848/187072391-9a043078-7508-44d5-96ec-72606bd94fb4.png)


### General coupling layer

![image](https://user-images.githubusercontent.com/43431848/187066801-81d40f62-3620-4391-8b2d-0755576b3fe0.png)


### Additive coupling layer

위에서의 coupling law $g$를 $g(a,b):=a+b$라고 정의하면 다음과 같이 쉽게 변환공식을 얻을 수 있다.

$$
\begin{align*}
y_{I_2} &= x_{I_2} + m(x_{I_1}) \\
x_{I_2} &= y_{I_2} - m(y_{I_1})
\end{align*}
$$

$m$은 $d$ input에서 $D-d$ output으로만 가는 아무 함수여도 된다. additive coupling layer에서는 $\det \dfrac{\partial y_{I_2}}{\partial x_{I_2}}=1$이므로 Jacobian이 항상 1이 됨을 알 수 있다.


### Combining coupling layers

위에서 알아본 coupling layers를 함성하여 충분히 복잡한(complex) transformation을 만들수 있다. 하나의 coupling layer에서는 input의 일부분이 안바뀌고 그대로 있기 때문에, 모든 dimension이 tranform되기 위해서는 이렇게 안바뀌는 부분의 역할을 바꿔서 넣는 것이 필요하다. 이 논문에서 Jacobian을 관찰한 결과로 최소한 3개의 coupling layer가 있어야 모든 dimension이 다른 dimension에 영향을 줄수 있는 것을 확인 하였다. 이런 맥락에서 이 논문에서는 4개의 coupling layer를 사용하였다.


## Allowing Rescaling

- Additive coupling layers들은 unit Jacobian determinant(volume preserving)이므로 모델의 복잡성을 주기 위해 Diagonal Scaling Matrix $S$를 top layer에 포함시켜서 $(x_i) \rightarrow (S_{ii} x_i)$ 를 통해서 각 dimension의 중요도를 이러한 weight $S$를 통하여 반영할 수 있게 하였다.
- 이를 이용하면 일부 $i$에 대하여 $S_{ii}$값이 굉장히 크면 그 부분의 hidden dimension은 중요하지 않다고 볼 수 있게 때문에 dimension reduction의 효과도 생길 수 있다.
- 이런 $S_{ii}$들은 데이터들의 PCA결과의 eigenvalue들과 연관시켜 생각할 수 있다. PCA를 한후에 eigenvalue가 큰 방향에서는 그 방향을 따라서 variation이 크므로 데이터에서 중요한 방향이라고 생각할 수 있는데, 여기에서는 $S_{ii}$가 크게 되면 반대로 $p_{H_i}(h_i)$가 미미하다고 생각할 수 있어서 그 방향의 중요도가 떨어진다고 볼수 있는 것이다.

따라서 최종적인 NICE Criterion은 다음과 같다.

$$
\log p_X(x) = \sum_{i=1}^{D} [ \log (p_{H_i} (f_i (x))) + \log (\lvert S_{ii} \lvert )]
$$

## Prior Distribution

$$
p_H(h) = \prod_{d=1}^D p_{H_d} (h_d)
$$
이 논문에서 $p_{H_d}$들은 gaussain distribution나 logistic distribution으로 정하였다. 특별히 logistic distribution을 사용했다고 한다. 그래디언트 측면에서 좋다고 한다.

# Experiments


## Log-likelihood and Image generation

![image](https://user-images.githubusercontent.com/43431848/187066722-fa525483-2b27-40f3-9e15-4a9dafa5e2f8.png)

![image](https://user-images.githubusercontent.com/43431848/187066731-7e92e365-cb19-47b3-afb0-16dd3c841492.png)


## Inpainting

![image](https://user-images.githubusercontent.com/43431848/187066753-ebf9d093-d4a0-40cd-946a-911f10162a1d.png)


# Conclusion

- 학습가능한 highly non-linear bijective 변환을 통해서 학습데이터의 분포를 factorized 분포를 갖는 공간으로 변환하는 모델을 제시하였다.
- 이는 MLE를 통해서 이루어진다.
- NICE는 efficent unbiased ancestral sampling을 할 수 있으며 이를 통해 높은 log-likelihood를 달성할 수 있었다.
- NICE를 통해 more complex family of approximate posterior를 학습할 수 있으며 더 다양한 prior를 이용할 수 있는 장점이 있다. 

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
