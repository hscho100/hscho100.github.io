---
layout: article
title: Neural Discrete Representation Learning
tags: Papers
key: 20220730
mode: immersive
excerpt_separator: <!--more-->
excerpt_type : html
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .8), rgba(139, 34, 139, .8))'
    src: /assets/dalle.png
---

# Abstract
전통적인 Text-to-image generation은 모델구조를 얼마나 잘 짜거나, domain-specific한 걸 이용해서(object part label, segmentation 등을 이용) 이루어졌는데 본 논문에서 autoregressive한 transformer를 기반으로 더 단순한 approach임에도 좋은 결과를 얻을 수 있었다.(물론 데이터는 충분해야함)

# Main Contributions
- Motivation
  - Lauguage는 그 특성상 inherently discrete하며, Image도 Language로 describe할 수 있기에 image도 discrete encoding(embedding)을 할 수 있다고 볼 수 있다.
  
1. Introduce a new family of generative models succesfully combining the VAE framework with discrete latent representations through a novel parametrization of the posterior distribution of (discrete) latents given an observation.
2. VQ-VAE is simple to train, does not suffer from large variance, and avoids the "posterior collapse" issue
  - Decoder $p_\theta(x\lvert z)$가 Powerful한 경우 latents가 무시된다. 
  - 위의 (2)번식을 보면 Decoder $p_\theta(x\lvert z)$가 있는 부분이 첫번째 term인 reconstruction term인 것을 알 수 있다. 
  - 다시 말해 각 $x$에 따라 $z$가 적당히 다르게만 encode되면(다시 표현하면 $x$의 intrinsic manifold특성이나 내부의 숨겨진 특징이 확률분포처럼 뭔가 meaningful한 것으로 encode되지 않고 그냥 아무 semantic이 반영되지 않는 z로 encode된다면) powerful decode가 그냥 $z$를 외워 그 $z$에 대응되는 $x$만 decode하는 방식으로 위의 (1), (2) ELBO식을 충분히 크게 Maximize 할 수 있다.
  - 즉, ELBO를 크게 하는 목표를 달성하지만 우리가 원하는 encode부분($q_\phi(z\lvert x)$)은 망할 수 있다는 것이다.(이를 "posterior collapse"라 하는 듯!)
3. VQ-VAE는 latent spacef를 effectively 사용한다.
4. Powerful Prior를 통해서 high quality하고 coherent한 이미지,스피치, 비디오 생성을 할 수 있다.(특히 raw speech에서 supervision없이 unsupervised speaker conversion을 해냈다) 

![vq_vae](/assets/dalle_fig1.png)

<!--more-->

# Method

- Transformer 이용하고 Stage 1, Stage 2로 구성

$$
\begin{align}
\ln p_{\theta, \psi} (x,y) \geq &= \mathbb{E}_{q_\phi(z\lvert x)}[ \ln p_\theta(x\lvert y,z)] - \beta KL(q_\phi(y,z\lvert x) \lvert \lvert p_\psi(y,z)) \\ 
                                 
\end{align}
$$

## Stage 1 : Learning the Visual Codebook

- dVAE1 이용하여 256×256 RGB image into a 32 × 32 grid of image tokens으로 압축하고 각 image token은 8192개의 categorical distribution의 outcome으로 되어 있다.
- This reduces the context size of the transformer by a factor of $192=8\times 8\times 3$ without a large degradation in visual quality.

## Stage 2 : Learning the Prior

- We concatenate up to 256 BPE-encoded text tokens with the 32 × 32 = 1024 image tokens.
- Train an autoregressive transformer to model the joint distribution over the text and image tokens.

### 구체적으로

- $\phi, \theta$를 구정하고 text와 image에 대한 prior distribution을 학습하였다(with respect to $\psi$)
- $p_\psi$는 120억 파라미터 갯수를 갖는 sparse transformer..
- text-image pair가 주어졌을 때 text같은 경우는 최대 256개의 토큰 & vocab_size of each token이 16384를 갖도록 lowercased caption에 BPE-encode를 했다.
- image같은 경우는 $32\times 32=1024$개의 토큰이며 한 토큰의 vocab_size는 8192인걸로 encode 했다.

- image token이 dVAE의 encoder의 logit output으로 argmax sampling을 했다.

- Transformer는 decoder only 모델을 썼으며 each image token이 64개의 self-attention layer중에 하나의 layer에서 모든 text-token에 대하여 attention을 계산.
## Notations


## Discrete Latent variables

$$
q(z=k\lvert x)= \begin{cases} 1 \quad \text{for } k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2, \\ 0 \quad \text{otherwise}\end{cases}
$$

$$
z_q(x)=e_k, \quad \text{where} \quad k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2
$$

### KL term in VAE?
KL term은 (2)식에서 보이는 것처럼 $KL(q_\phi(z\lvert x) \lvert \lvert p(z)) $인데 VQ-VAE에서는 보이지 않는다. 논문에서는 이 값이 상수($\log K$)라고 하는데 이는 간단히 보일수 있다. 먼저 prior $p(z)$가 논문에서 uniform categorical distribution이라고 뒀다는 걸 생각하면 discrete output $i=1,\cdots,K$ 각각에 대하여 모두 $p(z=i)=\frac{1}{K}$임을 생각하자. 또한 $q_\phi(z=k\lvert x)$가 위의 식에서 보이는 것처럼 argmin인 $z=k$에서만 1의 확률을 갖고 나머지 $z\neq k$에 대해서는 0의 확률을 갖는 one hot느낌의 categorical distribution을 갖는 posterior distribution임을 추가로 고려하면 다음과 같이 KL term이 계산된다.

$$
KL(q_\phi(z\lvert x) \lvert \lvert p(z)) = \sum_{z=1}^K q_\phi(z\lvert x) \log (q_\phi(z\lvert x) / p(z)) = 1*\log (1/(\frac{1}{K}))=\log K
$$

## Learning

$$
L = \log p(x\lvert z_q(x))+\lvert\lvert sg[z_e(x)]-e\lvert\lvert_2^2+\beta \lvert\lvert z_e(x)-sg[e]\lvert\lvert_2^2
$$

### Prior

먼저 prior $p(z)$를 uniform categorical distribution으로 고정해놓고 VQ-VAE의 모델을 학습시킨 다음에, Autoregressive model(e.g. PixelCNN, WaveNet)을 이용하여 p(z)를 autoregressive distribution이 되게 바꾼다음에 학습을 한다. 이때 $z$로 부터 $x$를 sampling(generate)할 때 ancestral sampling이 사용된다.

# Conclusion

1. 실제 활용이 가능한 text-to-image generation based on an autoregressive transformer을 개발하였다.
2. zero-shot performance relative to previous domain-specific approaches / range of capabilities that emerge from a single generative model 측면에서도 좋은결과를 냄.
3. 좋은 일반화 결과를 얻기 위해서 scale을 조절하면 된다.

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
