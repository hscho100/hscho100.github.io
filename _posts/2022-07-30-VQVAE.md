---
layout: article
title: Neural Discrete Representation Learning
tags: Papers
key: 20220730
mode: immersive
excerpt_separator: <!--more-->
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .8), rgba(139, 34, 139, .8))'
    src: /assets/vqvae.png
---
<!--more-->
# d

# Main Contributions
1. A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient method.
2. For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

![vae_graph](/assets/vae_graph.png)

![vq_vae](/assets/vqvae.png)

# VQ-VAE

- The posterior and prior distributions are categorical, and the samples drawn from these distributions index an embedding table.
- These embeddings are then used as input into the decoder network.

## Notations

- $e\in \mathbb{R}^{K\times D}$, $K$: the size of the discrete latent space, $D$: the dimension of each latent embedding vector $e_i$.
- ed

## Discrete Latent variables

$$
q(z=k\lvert x)= \begin{cases} 1 \quad \text{for } k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2, \\ 0 \quad \text{otherwise}\end{cases}
$$

$$
z_q(x)=e_k, \quad \text{where} \quad k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2
$$

## Learning

$$
L = \log p(x\lvert z_q(x))+\lvert\lvert sg[z_e(x)]-e\lvert\lvert_2^2+\beta \lvert\lvert z_e(x)-sg[e]\lvert\lvert_2^2
$$
1. Intractibility(보통 likelihood인 $p_\theta(x\|z)$가 복잡한 함수인 경우)

### Prior

먼저 prior $p(z)$를 uniform categorical distribution으로 고정해놓고 VQ-VAE의 모델을 학습시킨 다음에, Autoregressive model(e.g. PixelCNN, WaveNet)을 이용하여 p(z)를 autoregressive distribution이 되게 바꾼다음에 학습을 한다. 이때 $z$로 부터 $x$를 sampling(generate)할 때 ancestral sampling이 사용된다.

# Conclusion

1. VAEs with vector quantization to obtain a discrete latent representation
2. VQ-VAEs are capable of modelling very long term depedencies through their compressed discrete latent space
3. can model long range sequences and fully unsupervised learn high-level speech decriptors that are closely related to phonemes




---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
