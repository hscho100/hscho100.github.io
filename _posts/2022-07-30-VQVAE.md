---
layout: article
title: Neural Discrete Representation Learning
tags: Papers
key: 20220730
mode: immersive
excerpt_separator: <!--more-->
excerpt_type : html
article_header:
  type: overlay
  theme: dark
  background_color: '#203028'
  background_image:
    gradient: 'linear-gradient(135deg, rgba(34, 139, 87 , .8), rgba(139, 34, 139, .8))'
    src: /assets/vqvae.png
---

$$
\begin{align}
\mathcal{L}(\theta,\phi;x^{(i)}) &= \mathbb{E}_{q_\phi(z\lvert x)}[ -\log q_\phi(z\lvert x) + \log p_\theta(x,z) ] \\ 
                                 &= \mathbb{E}_{q_\phi(z\lvert x)}[ \log p_\phi(x\lvert z)) - KL(q_\phi(z\lvert x) \lvert \lvert p(z))
\end{align}
$$


# Main Contributions
1. Introduce a new family of generative models succesfully combining the VAE framework with discrete latent representations through a novel parametrization of the posterior distribution of (discrete) latents given an observation.
2. VQ-VAE is simple to train, does not suffer from large variance, and avoids the "posterior collapse" issue
- Decoder $p(x\lvert z)$ $q_\phi (z\lvert x)$가 Powerful한 경우 latents가 무시된다. 
3.t

![vae_graph](/assets/vae_graph.png)

![vq_vae](/assets/vqvae.png)

<!--more-->

# VQ-VAE

- The posterior and prior distributions are categorical, and the samples drawn from these distributions index an embedding table.
- These embeddings are then used as input into the decoder network.

## Notations

- $e\in \mathbb{R}^{K\times D}$, $K$: the size of the discrete latent space, $D$: the dimension of each latent embedding vector $e_i$.
- ed

## Discrete Latent variables

$$
q(z=k\lvert x)= \begin{cases} 1 \quad \text{for } k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2, \\ 0 \quad \text{otherwise}\end{cases}
$$

$$
z_q(x)=e_k, \quad \text{where} \quad k=\text{argmin}_j \lvert\lvert z_e(x) -e_j\lvert\lvert_2
$$

## Learning

$$
L = \log p(x\lvert z_q(x))+\lvert\lvert sg[z_e(x)]-e\lvert\lvert_2^2+\beta \lvert\lvert z_e(x)-sg[e]\lvert\lvert_2^2
$$
1. Intractibility(보통 likelihood인 $p_\theta(x\|z)$가 복잡한 함수인 경우)

### Prior

먼저 prior $p(z)$를 uniform categorical distribution으로 고정해놓고 VQ-VAE의 모델을 학습시킨 다음에, Autoregressive model(e.g. PixelCNN, WaveNet)을 이용하여 p(z)를 autoregressive distribution이 되게 바꾼다음에 학습을 한다. 이때 $z$로 부터 $x$를 sampling(generate)할 때 ancestral sampling이 사용된다.

# Conclusion

1. VAEs with vector quantization to obtain a discrete latent representation
2. VQ-VAEs are capable of modelling very long term depedencies through their compressed discrete latent space
3. can model long range sequences and fully unsupervised learn high-level speech decriptors that are closely related to phonemes




---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
