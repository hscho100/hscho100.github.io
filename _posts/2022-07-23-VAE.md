---
title: Auto-Encoding Variational Bayes
key: 20220723
tags: Papers
---

# Main Contributions
1. A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient method.
2. For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

![vae_graph](/_posts/vae_graph.png)

# Problem scenario

- $X=\\{ x^{(i)} \\}_{i=1}^N $ consisting of $N$ i.i.d. samples of some continuous or discrete variable $x$.
- $p_\theta(z), p_\theta(x\|z)$ come frome parametric families of distributions.
- $\theta$와 $z^{(i)}$은 모름
 
## 어려운 점들
1. Intractibility(보통 likelihood인 $p_\theta(x\|z)$가 복잡한 함수인 경우)
  - $p_\theta(x)=\int p_\theta(z)p_\theta(x\|z)$ 적분, or $\theta$에 대한 미분이 힘든 경우
  - posterior $p_\theta(z\|x)$ 가 intractable
  - mean-field VB algorithm에서 필요한 적분이 intractable한 경우
2. Large dataset
  - Sampling-based solution(e.g. Monte Carlo EM) 너무 느림(데이터하나 마다 샘플링을 하기 위한 반복루프필요)

## 3가지 문제를 풀자!

1. Efficient approximate ML or MAP estimation for the parameters θ. 이로 인해 위 그림의 Graph를 따르는 생성과정으로 좋은 결과(그림)을 만들기를 바람!
2. $x, \theta$가 주어졌을 때($x$같은 경우는 observed 되었을 때), 그에 대응되는 $z$를 효율적으로 잘 샘플링하거나 그 $z$에 대한 적분을 잘하고 싶다!(이를 Efficient approximate posterior inference $p(z\|x,\theta)$라고 함.
3. 마찬가지로 $x$에 관한 sampling 혹은 적분을 잘하고 싶음(Efficient approximate marginal inference). This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.

$q_\phi(z\lvert x), p_\theta(x\lvert z)$ 각각을 probabilistic encoder, probabilistic decoder라고 함.

## The variationial bound

$\log p_\theta(x^{(i)}) = D_KL()+\mathcal{L}(\theta,\phi;x^{(i)})$에서 $\mathcal{L}$ term
구체적으로

$$
\mathcal{L}(\theta,\phi;x^{(i)}) = \mathbb{E}_{q_\phi(z\lvert x)}[ -\log q_\phi(z\lvert x) + \log p_\theta(x,z) ]
$$

### Usual (naive) Monte Carlo gradient estimator

$$
\nabla_\phi \mathbb{E}_\{q_\phi(z)\}[f(z)] = \mathbb{E}_\{q_\phi(z)\}[f(Z)\nabla_\phi \log q_\phi(z) ]\approx \frac{1}{L} \sum_{l=1}^L f(z) \nabla_\phi \log q_\phi (z^{(l)}) 
$$

<!--more-->

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
