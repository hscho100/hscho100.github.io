---
title: Auto-Encoding Variational Bayes
tags: Papers
---

# Auto-Encoding Variational Bayes

## Main Contributions
1. A reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient method.
2. For i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.

![vae_graph](./vae_graph.png)

## Problem scenario

- $X=\\{ x^{(i)} \\}_{i=1}^N $ consisting of $N$ i.i.d. samples of some continuous or discrete variable $x$.
- $p_\theta(z), p_\theta(x|z)$ come frome parametric families of distributions.
- $\theta$와 $z^{(i)}$은 모름
 
### 어려운 점들
1. Intractibility
2. Large dataset

### 3가지 문제를 풀자!

1. Efficient approximate ML or MAP estimation for the parameters θ. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.
2. $x, \theta$가 주어졌을 때($x$같은 경우는 observed 되었을 때), 그에 대응되는 $z$를 효율적으로 잘 샘플링하거나 그 $z$에 대한 적분을 잘하고 싶다!(이를 Efficient approximate posterior inference $p(z|x,\theta)$라고 함
4. Efficient approximate marginal inference of the variable x. This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.



<!--more-->

---

If you like this post, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/hscho100/hscho100.github.io.svg?label=Stars&style=social)](https://github.com/hscho100/hscho100.github.io/)
